{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "b0604927-cb57-4860-b1a2-e90984ced2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorWithPadding\n",
    "from transformers.modeling_outputs import TokenClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "093fee09-2482-4dbd-ab2d-31845d9f5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/archive/\"\n",
    "data = \"Sarcasm_Headlines_Dataset_v2.json\"\n",
    "# with open(data_path, 'r', encoding='utf-8') as file:\n",
    "#     data = [json.loads(json_object) for json_object in file]\n",
    "# df = pd.json_normalize(data)\n",
    "\n",
    "df = pd.read_json(os.path.join(data_path, data), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "dc795802-b088-42d9-9ac7-a2b1300bcf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>1</td>\n",
       "      <td>jews to celebrate rosh hashasha or something</td>\n",
       "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28615</th>\n",
       "      <td>1</td>\n",
       "      <td>internal affairs investigator disappointed con...</td>\n",
       "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28616</th>\n",
       "      <td>0</td>\n",
       "      <td>the most beautiful acceptance speech this week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28617</th>\n",
       "      <td>1</td>\n",
       "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
       "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28618</th>\n",
       "      <td>1</td>\n",
       "      <td>dad clarifies this not a food stop</td>\n",
       "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28619 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                           headline  \\\n",
       "0                 1  thirtysomething scientists unveil doomsday clo...   \n",
       "1                 0  dem rep. totally nails why congress is falling...   \n",
       "2                 0  eat your veggies: 9 deliciously different recipes   \n",
       "3                 1  inclement weather prevents liar from getting t...   \n",
       "4                 1  mother comes pretty close to using word 'strea...   \n",
       "...             ...                                                ...   \n",
       "28614             1       jews to celebrate rosh hashasha or something   \n",
       "28615             1  internal affairs investigator disappointed con...   \n",
       "28616             0  the most beautiful acceptance speech this week...   \n",
       "28617             1  mars probe destroyed by orbiting spielberg-gat...   \n",
       "28618             1                 dad clarifies this not a food stop   \n",
       "\n",
       "                                            article_link  \n",
       "0      https://www.theonion.com/thirtysomething-scien...  \n",
       "1      https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2      https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3      https://local.theonion.com/inclement-weather-p...  \n",
       "4      https://www.theonion.com/mother-comes-pretty-c...  \n",
       "...                                                  ...  \n",
       "28614  https://www.theonion.com/jews-to-celebrate-ros...  \n",
       "28615  https://local.theonion.com/internal-affairs-in...  \n",
       "28616  https://www.huffingtonpost.com/entry/andrew-ah...  \n",
       "28617  https://www.theonion.com/mars-probe-destroyed-...  \n",
       "28618  https://www.theonion.com/dad-clarifies-this-no...  \n",
       "\n",
       "[28619 rows x 3 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ed5e55c4-bb2f-46b6-8e75-0e46e3dd8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c9454a13-f386-44ff-a3c6-6391b1954ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HF = load_dataset(path=data_path, data_files=data)\n",
    "\n",
    "dataset_HF = dataset_HF.remove_columns(['article_link'])\n",
    "\n",
    "dataset_HF.set_format('pandas')\n",
    "dataset_HF = dataset_HF['train'][:]\n",
    "dataset_HF.drop_duplicates(subset=['headline'],inplace=True)\n",
    "dataset_HF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataset_HF = Dataset.from_pandas(dataset_HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "10b54ae4-f819-4f77-8d2e-307fbd4fb08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['is_sarcastic', 'headline'],\n",
       "    num_rows: 28503\n",
       "})"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "60fe6983-47f2-44fb-8d3d-7a7fee061fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset_HF.train_test_split(test_size=0.2, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "69512110-e7d1-4b74-86f1-b9b86359700a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['is_sarcastic', 'headline'],\n",
       "        num_rows: 22802\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['is_sarcastic', 'headline'],\n",
       "        num_rows: 5701\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8de26f65-018e-4ec3-a5b7-1656edbd687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "767a19d8-6e0d-4785-bc95-93b587d14194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['is_sarcastic', 'headline'],\n",
       "        num_rows: 2850\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['is_sarcastic', 'headline'],\n",
       "        num_rows: 2851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1180a483-d9d7-4125-9633-3012f8a27705",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HF = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': valid_test_split['train'],\n",
    "    'test': valid_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4a07e93a-b2f5-4523-9dbd-2ae3016acf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['is_sarcastic', 'headline'],\n",
       "        num_rows: 22802\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['is_sarcastic', 'headline'],\n",
       "        num_rows: 2850\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['is_sarcastic', 'headline'],\n",
       "        num_rows: 2851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f10cf031-8bb4-4134-ae73-c778f13bdcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT=\"distilbert-base-uncased\"\n",
    "MAX_SEQUENCE_LENGTH=512\n",
    "EMBED_VECTOR_SIZE=768\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "tokenizer.model_max_length = MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f3279059-5e8a-4560-b6da-ec089e8a4be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ca5501c29341dba4cb6ce91933e95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f405ed47e3c423497b58020841bc3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa6590713e54a91a5a862661f4710ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['headline'], truncation=True, max_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "tokenized_dataset = dataset_HF.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "6d853aa0-f0c9-4687-b472-9825c38a057f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['is_sarcastic', 'headline', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 22802\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['is_sarcastic', 'headline', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2850\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['is_sarcastic', 'headline', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2851\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ef95bc86-1f51-4fc1-98d8-b06a02dbf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'is_sarcastic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "30b7e0bf-4b45-4276-a2c1-b359d805eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCollator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e885342c-a684-48d4-ae31-9945f7b38a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128 \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset['train'], shuffle = True, batch_size = BATCH_SIZE, collate_fn = dataCollator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_dataset['validation'], shuffle = True, collate_fn = dataCollator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "fb5b6b22-34d0-417f-a215-0441189f4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customized_Network(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        super(Customized_Network, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.model = AutoModel.from_pretrained(checkpoint, config=AutoConfig.from_pretrained(checkpoint, \n",
    "                                                                                             output_attention=True, \n",
    "                                                                                             output_hidden_state=True\n",
    "                                                                                            ))\n",
    "                # New Layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels )\n",
    "        \n",
    "    def forward(self, input_ids = None, attention_mask=None, is_sarcastic = None ):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids (torch.Tensor, optional): Tensor of input IDs. Defaults to None.\n",
    "            attention_mask (torch.Tensor, optional): Tensor for attention masks. Defaults to None.\n",
    "            labels (torch.Tensor, optional): Tensor for labels. Defaults to None.\n",
    "            \n",
    "        Returns:\n",
    "            TokenClassifierOutput: A named tuple with the following fields:\n",
    "            - loss (torch.FloatTensor of shape (1,), optional, returned when label_ids is provided) â€“ Classification loss.\n",
    "            - logits (torch.FloatTensor of shape (batch_size, num_labels)) â€“ Classification scores before SoftMax.\n",
    "            - hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) \n",
    "            â€“ Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
    "            - attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True) \n",
    "            â€“ Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids = input_ids, attention_mask = attention_mask  )\n",
    "        \n",
    "        last_hidden_state = outputs[0]\n",
    "        \n",
    "        sequence_outputs = self.dropout(last_hidden_state)\n",
    "        \n",
    "        logits = self.classifier(sequence_outputs[:, 0, : ].view(-1, 768 ))\n",
    "        \n",
    "        loss = None\n",
    "        loss = None\n",
    "        if is_sarcastic is not None:\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            loss = loss_func(logits.view(-1, self.num_labels), is_sarcastic.view(-1))\n",
    "            \n",
    "            return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "daea9900-e3d3-431d-8c97-3ee09c4af6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Customized_Network(checkpoint=CHECKPOINT, num_labels=2 ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "840907a5-05a3-47db-9d61-4da448746165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mahno\\Anaconda\\envs\\env\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5 )\n",
    "\n",
    "num_epoch = 3\n",
    "num_training_steps = num_epoch * len(train_dataloader)\n",
    "num_warmup_steps = 0 * num_training_steps  # You can adjust the warm-up proportion as needed\n",
    "\n",
    "# Create the scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8cc05ffd-d101-4730-8094-bf826f4d9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "e35fe92c-1877-44aa-b8fc-28a22c0d4436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759aa69567f34d1a8dd97c11174a5f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a24afa2be054aa8b728e7ffd1278805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.8979433449747768}\n",
      "{'f1': 0.9313653136531366}\n",
      "{'f1': 0.9307295504789979}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epoch * len(eval_dataloader) ))\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar_train.update(1)\n",
    "        \n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim = -1 )\n",
    "        metric.add_batch(predictions = predictions, references = batch['is_sarcastic'] )\n",
    "        progress_bar_eval.update(1)\n",
    "        \n",
    "    print(metric.compute()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "4b02dfb0-a8b0-4e35-87f1-3d5944d1d324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9263000374111485}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_dataset['test'], batch_size = 32, collate_fn = dataCollator\n",
    ")\n",
    "\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = { k: v.to(device) for k, v in batch.items() }\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim = -1)\n",
    "    metric.add_batch(predictions=predictions, references=batch['is_sarcastic'])\n",
    "\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "11df0a97-e560-492f-a8a1-5f552cfd47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "3e8544f3-69f0-400d-a330-91e14786ed55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\vocab.txt',\n",
       " 'tokenizer\\\\added_tokens.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae924c-692f-4961-8905-e1f1f649cdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
